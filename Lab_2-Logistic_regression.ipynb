{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nML lab No1\\nHomework on Machine Learning Technologies.\\nImplementing of stochastic gradient descent\\nand Adam's optimization algorithms using numpy library.\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "ML lab No2\n",
    "Homework on Machine Learning Technologies.\n",
    "Applying the logistic regression method using different optimize functions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task:\n",
    "1.\tApply the logistic regression method using the functions in the notebook «Logistic Regression as a Neural Network – BP alg.ipynb” to predict the biological response of a molecule. Use 75% of the sample to train the model, and the rest of the data to estimate its accuracy.\n",
    "2.\tModify optimize() function to implement the stochastic gradient descent (SGD) method. Apply it to solve the problem from p.1.\n",
    "3.\tFor two modifications of gradient descent (pp. 1 and 2), plot the learning curves (dependence of the value of the loss function on the iteration number), apply models with different values of the learning rate (at least 5 different learning rates). How does it affect the accuracy of the model?\n",
    "4.\t*(not nesessary) Implement the Adam optimization method using the numpy library and compare the accuracy of the model trained with it with the models trained by the classic GD and SGD algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression method functions copied from\n",
    "# JNB 'Logistic Regression as a Neural Network - BP alg'):\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "    s = 1./(1.+np.exp(-z))\n",
    "    \n",
    "    return s\n",
    "\n",
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    w = np.zeros((dim,1))\n",
    "    b = 0.\n",
    "    \n",
    "    return w, b\n",
    "\n",
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size which equals the number of features\n",
    "    b -- bias, a scalar\n",
    "    X -- data \n",
    "    Y -- true \"label\" vector (containing 0 and 1) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    #print('number of objects = ',len(X))\n",
    "    \n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    A = sigmoid(np.dot(w.T,X)+b )                                 # compute activation\n",
    "    cost = -(1./m)*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A),axis=1)   # compute cost\n",
    "    \n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    dw = (1./m)*np.dot(X,(A-Y).T)\n",
    "    db = (1./m)*np.sum(A-Y,axis=1)\n",
    "#     if np.sum(w) == 0:\n",
    "#         print('A shape: ', A.shape)\n",
    "#         print('X shape: ', X.shape)\n",
    "#         print('Y shape: ', Y.shape)\n",
    "#         print('dw shape: ', dw.shape)\n",
    "#         print('db shape: ', db.shape)\n",
    "\n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost\n",
    "\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array \n",
    "    b -- bias, a scalar\n",
    "    X -- data \n",
    "    Y -- true \"label\" vector (containing 0 and 1), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "                \n",
    "        # Cost and gradient calculation \n",
    "        grads, cost = propagate(w,b,X,Y)\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule\n",
    "        w -=learning_rate*dw\n",
    "        b -=learning_rate*db\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs\n",
    "\n",
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array \n",
    "    b -- bias, a scalar\n",
    "    X -- data \n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities \n",
    "    A = sigmoid(np.dot(w.T,X)+b)\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        \n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        if (A[0,i]<=0.5):\n",
    "            Y_prediction[0][i]=0\n",
    "        else:\n",
    "            Y_prediction[0][i]=1\n",
    "    \n",
    "    return Y_prediction\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000,\n",
    "          learning_rate = 0.5, print_cost = False, optimize = optimize):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function we've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array \n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array \n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize parameters with zeros \n",
    "    w, b = initialize_with_zeros(X_train.shape[0])\n",
    "\n",
    "    # Gradient descent\n",
    "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    # Predict test/train set examples\n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Activity</th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>D5</th>\n",
       "      <th>D6</th>\n",
       "      <th>D7</th>\n",
       "      <th>D8</th>\n",
       "      <th>D9</th>\n",
       "      <th>...</th>\n",
       "      <th>D1767</th>\n",
       "      <th>D1768</th>\n",
       "      <th>D1769</th>\n",
       "      <th>D1770</th>\n",
       "      <th>D1771</th>\n",
       "      <th>D1772</th>\n",
       "      <th>D1773</th>\n",
       "      <th>D1774</th>\n",
       "      <th>D1775</th>\n",
       "      <th>D1776</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.497009</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.132956</td>\n",
       "      <td>0.678031</td>\n",
       "      <td>0.273166</td>\n",
       "      <td>0.585445</td>\n",
       "      <td>0.743663</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.606291</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111209</td>\n",
       "      <td>0.803455</td>\n",
       "      <td>0.106105</td>\n",
       "      <td>0.411754</td>\n",
       "      <td>0.836582</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.033300</td>\n",
       "      <td>0.480124</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.209791</td>\n",
       "      <td>0.610350</td>\n",
       "      <td>0.356453</td>\n",
       "      <td>0.517720</td>\n",
       "      <td>0.679051</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538825</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.196344</td>\n",
       "      <td>0.724230</td>\n",
       "      <td>0.235606</td>\n",
       "      <td>0.288764</td>\n",
       "      <td>0.805110</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.517794</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.494734</td>\n",
       "      <td>0.781422</td>\n",
       "      <td>0.154361</td>\n",
       "      <td>0.303809</td>\n",
       "      <td>0.812646</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1777 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Activity        D1        D2    D3   D4        D5        D6        D7  \\\n",
       "0         1  0.000000  0.497009  0.10  0.0  0.132956  0.678031  0.273166   \n",
       "1         1  0.366667  0.606291  0.05  0.0  0.111209  0.803455  0.106105   \n",
       "2         1  0.033300  0.480124  0.00  0.0  0.209791  0.610350  0.356453   \n",
       "3         1  0.000000  0.538825  0.00  0.5  0.196344  0.724230  0.235606   \n",
       "4         0  0.100000  0.517794  0.00  0.0  0.494734  0.781422  0.154361   \n",
       "\n",
       "         D8        D9  ...  D1767  D1768  D1769  D1770  D1771  D1772  D1773  \\\n",
       "0  0.585445  0.743663  ...      0      0      0      0      0      0      0   \n",
       "1  0.411754  0.836582  ...      1      1      1      1      0      1      0   \n",
       "2  0.517720  0.679051  ...      0      0      0      0      0      0      0   \n",
       "3  0.288764  0.805110  ...      0      0      0      0      0      0      0   \n",
       "4  0.303809  0.812646  ...      0      0      0      0      0      0      0   \n",
       "\n",
       "   D1774  D1775  D1776  \n",
       "0      0      0      0  \n",
       "1      0      1      0  \n",
       "2      0      0      0  \n",
       "3      0      0      0  \n",
       "4      0      0      0  \n",
       "\n",
       "[5 rows x 1777 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir = \"D:/Work/Data_files/working_dir/\"\n",
    "file = \"bioresponse.csv\"\n",
    "\n",
    "df = pd.read_csv(dir + file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train/test splitted samples of required shape:\n",
    "\n",
    "y = df['Activity']\n",
    "X = df.drop('Activity', axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25)\n",
    "\n",
    "X_train = np.array(X_train.T)\n",
    "X_test = np.array(X_test.T)\n",
    "y_train = np.array([y_train])\n",
    "y_test = np.array([y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.680294\n",
      "Cost after iteration 200: 0.673197\n",
      "Cost after iteration 300: 0.666863\n",
      "Cost after iteration 400: 0.660968\n",
      "Cost after iteration 500: 0.655450\n",
      "Cost after iteration 600: 0.650269\n",
      "Cost after iteration 700: 0.645391\n",
      "Cost after iteration 800: 0.640787\n",
      "Cost after iteration 900: 0.636431\n",
      "Cost after iteration 1000: 0.632302\n",
      "Cost after iteration 1100: 0.628379\n",
      "Cost after iteration 1200: 0.624646\n",
      "Cost after iteration 1300: 0.621086\n",
      "Cost after iteration 1400: 0.617688\n",
      "Cost after iteration 1500: 0.614437\n",
      "Cost after iteration 1600: 0.611324\n",
      "Cost after iteration 1700: 0.608337\n",
      "Cost after iteration 1800: 0.605469\n",
      "Cost after iteration 1900: 0.602711\n",
      "train accuracy: 72.69818698897973 %\n",
      "test accuracy: 71.53518123667376 %\n"
     ]
    }
   ],
   "source": [
    "# 1. Logistic regression method using the functions\n",
    "# in the notebook «Logistic Regression...”\n",
    "\n",
    "d = model(X_train, y_train, X_test, y_test, num_iterations = 2000, learning_rate = 0.001, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Modify optimize() function to implement\n",
    "# the stochastic gradient descent (SGD) method\n",
    "\n",
    "def propagate_SGD(w, b, X, Y, iteration):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its partial gradient\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size which equals the number of features\n",
    "    b -- bias, a scalar\n",
    "    X -- data \n",
    "    Y -- true \"label\" vector (containing 0 and 1) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- partial gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- partial gradient of the loss with respect to b, thus same shape as b\n",
    "    \n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    if iteration ==  0:\n",
    "        # compute activation on full batch:\n",
    "        A = sigmoid(np.dot(w.T, X) + b)\n",
    "    else:\n",
    "        # change previously computed activation:\n",
    "        A = sigmoid(np.dot(w.T, X) + b)\n",
    "    cost = -(1./m) * np.sum(Y * np.log(A) +\n",
    "                            (1-Y) * np.log(1-A), axis=1)\n",
    "    \n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    dw = (1./m)*np.dot(X[iteration%len(w)],(A-Y).T)\n",
    "    db = (1./m)*np.sum(A-Y,axis=1)\n",
    "\n",
    "#    if np.sum(w) == 0:\n",
    "#         print('A shape: ', A.shape)\n",
    "#         print('A: ', A)\n",
    "#         print('X shape: ', X.shape)\n",
    "#         print('Y shape: ', Y.shape)\n",
    "#         print('Y: ', Y)\n",
    "#         print('dw shape: ', dw.shape)\n",
    "#         print('dw: ', dw)\n",
    "#         print('db shape: ', db.shape)\n",
    "#         print('db: ', db)\n",
    "#         print('w[iteration%len(w)][0]: ', w[iteration%len(w)][0])\n",
    "#         print('len(X[iteration]): ', len(X[iteration]))\n",
    "\n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost\n",
    "\n",
    "def optimize_SGD(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a\n",
    "    stochastic gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array \n",
    "    b -- bias, a scalar\n",
    "    X -- data \n",
    "    Y -- true \"label\" vector (containing 0 and 1), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "                \n",
    "        # Cost and gradient calculation \n",
    "        grads, cost = propagate_SGD(w, b, X, Y, i)\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule\n",
    "        w[i%len(w)][0] -= learning_rate * dw\n",
    "        b -= learning_rate * db\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 'step' training iterations:\n",
    "        step = 10000\n",
    "        if print_cost and i % step == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 2000: 0.687737\n",
      "Cost after iteration 4000: 0.686731\n",
      "Cost after iteration 6000: 0.685735\n",
      "Cost after iteration 8000: 0.684768\n",
      "Cost after iteration 10000: 0.683406\n",
      "Cost after iteration 12000: 0.682291\n",
      "Cost after iteration 14000: 0.681344\n",
      "Cost after iteration 16000: 0.680450\n",
      "Cost after iteration 18000: 0.679386\n",
      "Cost after iteration 20000: 0.678534\n",
      "Cost after iteration 22000: 0.677686\n",
      "Cost after iteration 24000: 0.676860\n",
      "Cost after iteration 26000: 0.675674\n",
      "Cost after iteration 28000: 0.674743\n",
      "Cost after iteration 30000: 0.673932\n",
      "Cost after iteration 32000: 0.673030\n",
      "Cost after iteration 34000: 0.672214\n",
      "Cost after iteration 36000: 0.671467\n",
      "Cost after iteration 38000: 0.670726\n",
      "Cost after iteration 40000: 0.670001\n",
      "Cost after iteration 42000: 0.668943\n",
      "Cost after iteration 44000: 0.668137\n",
      "Cost after iteration 46000: 0.667424\n",
      "Cost after iteration 48000: 0.666612\n",
      "Cost after iteration 50000: 0.665883\n",
      "Cost after iteration 52000: 0.665214\n",
      "Cost after iteration 54000: 0.664550\n",
      "Cost after iteration 56000: 0.663898\n",
      "Cost after iteration 58000: 0.662929\n",
      "Cost after iteration 60000: 0.662222\n",
      "Cost after iteration 62000: 0.661583\n",
      "Cost after iteration 64000: 0.660826\n",
      "Cost after iteration 66000: 0.660175\n",
      "Cost after iteration 68000: 0.659566\n",
      "Cost after iteration 70000: 0.658961\n",
      "Cost after iteration 72000: 0.658339\n",
      "Cost after iteration 74000: 0.657470\n",
      "Cost after iteration 76000: 0.656836\n",
      "Cost after iteration 78000: 0.656251\n",
      "Cost after iteration 80000: 0.655549\n",
      "Cost after iteration 82000: 0.654949\n",
      "Cost after iteration 84000: 0.654388\n",
      "Cost after iteration 86000: 0.653827\n",
      "Cost after iteration 88000: 0.653228\n",
      "Cost after iteration 90000: 0.652443\n",
      "Cost after iteration 92000: 0.651865\n",
      "Cost after iteration 94000: 0.651325\n",
      "Cost after iteration 96000: 0.650665\n",
      "Cost after iteration 98000: 0.650108\n",
      "Cost after iteration 100000: 0.649587\n",
      "Cost after iteration 102000: 0.649064\n",
      "Cost after iteration 104000: 0.648478\n",
      "Cost after iteration 106000: 0.647776\n",
      "Cost after iteration 108000: 0.647234\n",
      "Cost after iteration 110000: 0.646732\n",
      "Cost after iteration 112000: 0.646108\n",
      "Cost after iteration 114000: 0.645589\n",
      "Cost after iteration 116000: 0.645100\n",
      "Cost after iteration 118000: 0.644610\n",
      "Cost after iteration 120000: 0.644021\n",
      "Cost after iteration 122000: 0.643402\n",
      "Cost after iteration 124000: 0.642892\n",
      "Cost after iteration 126000: 0.642422\n",
      "Cost after iteration 128000: 0.641828\n",
      "Cost after iteration 130000: 0.641342\n",
      "Cost after iteration 132000: 0.640881\n",
      "Cost after iteration 134000: 0.640419\n",
      "Cost after iteration 136000: 0.639853\n",
      "Cost after iteration 138000: 0.639280\n",
      "Cost after iteration 140000: 0.638801\n",
      "Cost after iteration 142000: 0.638356\n",
      "Cost after iteration 144000: 0.637791\n",
      "Cost after iteration 146000: 0.637333\n",
      "Cost after iteration 148000: 0.636897\n",
      "Cost after iteration 150000: 0.636459\n",
      "Cost after iteration 152000: 0.635906\n",
      "Cost after iteration 154000: 0.635382\n",
      "Cost after iteration 156000: 0.634928\n",
      "Cost after iteration 158000: 0.634507\n",
      "Cost after iteration 160000: 0.633965\n",
      "Cost after iteration 162000: 0.633533\n",
      "Cost after iteration 164000: 0.633117\n",
      "Cost after iteration 166000: 0.632704\n",
      "Cost after iteration 168000: 0.632169\n",
      "Cost after iteration 170000: 0.631680\n",
      "Cost after iteration 172000: 0.631251\n",
      "Cost after iteration 174000: 0.630850\n",
      "Cost after iteration 176000: 0.630332\n",
      "Cost after iteration 178000: 0.629922\n",
      "Cost after iteration 180000: 0.629526\n",
      "Cost after iteration 182000: 0.629132\n",
      "Cost after iteration 184000: 0.628608\n",
      "Cost after iteration 186000: 0.628156\n",
      "Cost after iteration 188000: 0.627750\n",
      "Cost after iteration 190000: 0.627368\n",
      "Cost after iteration 192000: 0.626870\n",
      "Cost after iteration 194000: 0.626481\n",
      "Cost after iteration 196000: 0.626103\n",
      "Cost after iteration 198000: 0.625727\n",
      "train accuracy: 68.92996800568787 %\n",
      "test accuracy: 67.05756929637528 %\n"
     ]
    }
   ],
   "source": [
    "SGD_mod = model(X_train, y_train, X_test, y_test, num_iterations = 100000,\n",
    "                learning_rate = 0.01, print_cost = True, optimize = optimize_SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
